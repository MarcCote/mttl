{   
    "description": "50 selected with 0.5 threshold from gpt neo 1.3B embeddings",
    "selected50": {
        "duorc_SelfRC_build_story_around_qa, MATH/PRM-800K": 0.3301,
        "duorc_SelfRC_build_story_around_qa, niv2_summarization": 0.3445,
        "dream_answer_to_dialogue, coqa_1_0_0": 0.3575,
        "ultrachat_11, duorc_ParaphraseRC_build_story_around_qa": 0.3623,
        "niv2_question_decomposition, cot_ecqa": 0.3836,
        "ultrachat_15, coqa_1_0_0": 0.3976,
        "niv2_text_simplification, niv2_grammar_error_correction": 0.4054,
        "dream_answer_to_dialogue, ultrachat_23": 0.4279,
        "wiki_bio_who, para_crawl_enes": 0.4335,
        "wiki_bio_key_content, cot_creak": 0.4491,
        "duorc_SelfRC_build_story_around_qa, adversarial_qa_dbert_tell_what_it_is": 0.4688,
        "fix_punct, MATH/PRM-800K": 0.4823,
        "race_high_Read_the_article_and_answer_the_question_no_option_, duorc_SelfRC_build_story_around_qa": 0.485,
        "coqa_1_0_0, ropes_background_situation_middle": 0.5025,
        "glue_sst2_2_0_0, niv2_summarization": 0.5162,
        "super_glue_wic_1_0_2, niv2_grammar_error_correction": 0.527,
        "professional_law, niv2_summarization": 0.5498,
        "yelp_polarity_reviews_0_2_0, para_crawl_enes": 0.5584,
        "miscellaneous, ultrachat_11": 0.5709,
        "duorc_SelfRC_answer_question, ultrachat_22": 0.5833,
        "dream_generate_last_utterance, niv2_sentence_ordering": 0.5959,
        "cot_gsm8k, unified_qa_science_inst": 0.6062,
        "college_mathematics, cos_e_v1_11_i_think": 0.6308,
        "duorc_ParaphraseRC_title_generation, web_questions_get_the_answer": 0.6445,
        "niv2_question_rewriting, ropes_given_background_situation": 0.6531,
        "web_questions_whats_the_answer, quartz_given_the_fact_answer_the_q": 0.6721,
        "adversarial_qa_dbidaf_generate_question, duorc_SelfRC_title_generation": 0.6802,
        "ropes_background_new_situation_answer, gem_common_gen_1_1_0": 0.6927,
        "ultrachat_20, machine_learning": 0.7083,
        "cos_e_v1_11_question_description_option_text, wiki_hop_original_generate_subject": 0.7255,
        "duorc_SelfRC_generate_question, ropes_background_situation_middle": 0.7397,
        "lambada_1_0_0, cos_e_v1_11_i_think": 0.7453,
        "glue_sst2_2_0_0, race_middle_Write_a_multi_choice_question_options_given_": 0.7618,
        "race_middle_Select_the_best_answer, ultrachat_4": 0.7767,
        "cos_e_v1_11_question_option_description_text, quoref_Found_Context_Online": 0.7879,
        "niv2_toxic_language_detection, quail_no_prompt_text": 0.807,
        "niv2_ethics_classification, adversarial_qa_dbert_based_on": 0.8107,
        "app_reviews_generate_review, ultrachat_12": 0.8331,
        "marketing, gem_e2e_nlg_1_1_0": 0.8481,
        "high_school_world_history, social_i_qa_Generate_the_question_from_the_answer": 0.8537,
        "nutrition, ultrachat_27": 0.8727,
        "public_relations, wiki_qa_Topic_Prediction_Question_and_Answer_Pair": 0.891,
        "niv2_paraphrasing, glue_qqp_2_0_0": 0.8952,
        "computer_security, machine_learning": 0.9179,
        "race_middle_Is_this_the_right_answer, niv2_word_analogy": 0.9314,
        "cos_e_v1_11_question_option_description_id, scibench": 0.944,
        "race_middle_Is_this_the_right_answer, niv2_code_to_text": 0.9537,
        "niv2_paraphrasing, niv2_fill_in_the_blank": 0.9688,
        "niv2_cause_effect_classification, niv2_gender_classification": 0.9803,
        "quartz_given_the_fact_answer_the_q, quartz_paragraph_question_plain_concat": 0.9985
    },
    "unique_tasks_part1": [
        "MATH/PRM-800K",
        "adversarial_qa_dbert_based_on",
        "adversarial_qa_dbert_tell_what_it_is",
        "coqa_1_0_0",
        "cos_e_v1_11_i_think",
        "cot_creak",
        "cot_ecqa",
        "duorc_ParaphraseRC_build_story_around_qa",
        "duorc_SelfRC_build_story_around_qa",
        "duorc_SelfRC_title_generation",
        "gem_common_gen_1_1_0",
        "gem_e2e_nlg_1_1_0",
        "glue_qqp_2_0_0",
        "machine_learning",
        "niv2_code_to_text"],
    "unique_tasks_part2": [
        "niv2_fill_in_the_blank",
        "niv2_gender_classification",
        "niv2_grammar_error_correction",
        "niv2_sentence_ordering",
        "niv2_summarization",
        "niv2_word_analogy",
        "para_crawl_enes",
        "quail_no_prompt_text",
        "quartz_given_the_fact_answer_the_q",
        "quartz_paragraph_question_plain_concat",
        "quoref_Found_Context_Online",
        "race_middle_Write_a_multi_choice_question_options_given_",
        "ropes_background_situation_middle",
        "ropes_given_background_situation",
        "scibench"
    ],
    "unique_tasks_part3": [
        "social_i_qa_Generate_the_question_from_the_answer",
        "ultrachat_11",
        "ultrachat_12",
        "ultrachat_22",
        "ultrachat_23",
        "ultrachat_27",
        "ultrachat_4",
        "unified_qa_science_inst",
        "web_questions_get_the_answer",
        "wiki_hop_original_generate_subject",
        "wiki_qa_Topic_Prediction_Question_and_Answer_Pair",
        "adversarial_qa_dbidaf_generate_question",
        "app_reviews_generate_review",
        "college_mathematics",
        "computer_security",
        "coqa_1_0_0"],
    "unique_tasks_part4": [
        "cos_e_v1_11_question_description_option_text",
        "cos_e_v1_11_question_option_description_id",
        "cos_e_v1_11_question_option_description_text",
        "cot_gsm8k",
        "dream_answer_to_dialogue",
        "dream_generate_last_utterance",
        "duorc_ParaphraseRC_title_generation",
        "duorc_SelfRC_answer_question",
        "duorc_SelfRC_build_story_around_qa",
        "duorc_SelfRC_generate_question",
        "fix_punct",
        "glue_sst2_2_0_0",
        "high_school_world_history",
        "lambada_1_0_0",
        "marketing"],
    "unique_tasks_part5": [
        "miscellaneous",
        "niv2_cause_effect_classification",
        "niv2_ethics_classification",
        "niv2_paraphrasing",
        "niv2_question_decomposition",
        "niv2_question_rewriting",
        "niv2_text_simplification",
        "niv2_toxic_language_detection",
        "nutrition",
        "professional_law",
        "public_relations",
        "quartz_given_the_fact_answer_the_q"],
    "unique_tasks_part6": [
        "race_high_Read_the_article_and_answer_the_question_no_option_",
        "race_middle_Is_this_the_right_answer",
        "race_middle_Select_the_best_answer",
        "ropes_background_new_situation_answer",
        "super_glue_wic_1_0_2",
        "ultrachat_11",
        "ultrachat_15",
        "ultrachat_20",
        "web_questions_whats_the_answer",
        "wiki_bio_key_content",
        "wiki_bio_who",
        "yelp_polarity_reviews_0_2_0"
    ]
}