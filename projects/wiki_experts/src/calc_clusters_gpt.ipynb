{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/comp_3.9/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/anaconda/envs/comp_3.9/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda9SetDeviceEi'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering modifier...lora\n",
      "Registering modifier...poly\n",
      "Registering modifier...per_token_poly\n",
      "Registering modifier...skilled\n",
      "Registering modifier...kv_adapter\n",
      "Registering modifier...poly_kv_adapter\n",
      "Registering modifier...prompt_tuning\n",
      "Registering modifier...poly_prompt_tuning\n",
      "Registering modifier...hard_prompt\n",
      "Registering multi-expert selector...poly_router\n",
      "Registering multi-expert selector...moe_rkhs_router\n",
      "Registering multi-expert selector...zero_router\n",
      "Registering multi-expert selector...zero_per_token_router\n",
      "Registering multi-expert selector...poly_router_dir\n",
      "Registering multi-expert selector...info_selector\n",
      "Registering multi-expert selector...task_selector\n",
      "Registering multi-expert selector...kv_task_selector\n",
      "Registering multi-expert selector...kv_concat_selector\n",
      "Registering multi-expert selector...kv_norm_selector\n",
      "Registering multi-expert selector...kv_concat_norm_selector\n",
      "Registering multi-expert selector...kv_task_norm_selector\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "from collections import defaultdict\n",
    "from projects.wiki_experts.src.evolution.utils import get_svd_embedding\n",
    "\n",
    "from mttl.models.modifiers.expert_containers.expert_library import LocalExpertLibrary,  HFExpertLibrary\n",
    "from mttl.models.modifiers.expert_containers.library_transforms import SVDEmbeddingTransform, SVDEmbeddingTransformConfig\n",
    "from huggingface_hub import login, HfApi\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create clsuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/v-oostapenko/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "hf_api_key = os.environ[\"HF_TOKEN\"]\n",
    "login(token=hf_api_key)\n",
    "user = HfApi(token=hf_api_key).whoami()\n",
    "hf_repo_id = \"ostapeno/library-gptneo_1B_flan_2ep\"\n",
    "local_lib_location = f\"/tmp/{hf_repo_id}\"\n",
    "if not os.path.exists(local_lib_location):\n",
    "    os.makedirs(local_lib_location)\n",
    "    expert_lib: LocalExpertLibrary = LocalExpertLibrary.create_from_remote(\n",
    "        HFExpertLibrary(hf_repo_id), local_lib_location\n",
    "    )\n",
    "else:\n",
    "    expert_lib: LocalExpertLibrary = LocalExpertLibrary(local_lib_location)\n",
    "\n",
    "import json\n",
    "task_set_path = \"/home/v-oostapenko/dev/lucas_mttl/projects/wiki_experts/task_sets/flan_tasks.json\"\n",
    "flan256 = json.load(open(task_set_path))[\"flan256\"]\n",
    "        \n",
    "for expert_name, expert in list(expert_lib.data.items()):\n",
    "    if expert.expert_task_name not in flan256:\n",
    "        expert_lib.remove_expert(expert_name)\n",
    "assert len(expert_lib) == 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"glue_sst2_2_0_0\"\n",
      "\"dream_read_the_following_conversation_and_answer_the_question\"\n",
      "\"race_middle_Read_the_article_and_answer_the_question_no_option_\"\n",
      "\"adversarial_qa_droberta_generate_question\"\n",
      "\"adversarial_qa_dbidaf_question_context_answer\"\n",
      "\"app_reviews_convert_to_star_rating\"\n",
      "\"race_high_Select_the_best_answer\"\n",
      "\"super_glue_rte_1_0_2\"\n",
      "\"true_case\"\n",
      "\"wiqa_what_might_be_the_first_step_of_the_process\"\n",
      "\"quail_description_context_question_answer_id\"\n",
      "\"quail_context_question_description_text\"\n",
      "\"stream_qed\"\n",
      "\"huggingface_xsum\"\n",
      "\"cos_e_v1_11_question_option_description_text\"\n",
      "\"wiqa_what_is_the_final_step_of_the_following_process\"\n",
      "\"ropes_background_new_situation_answer\"\n",
      "\"wiki_qa_found_on_google\"\n",
      "\"cot_esnli\"\n",
      "\"social_i_qa_Show_choices_and_generate_answer\"\n",
      "\"cot_gsm8k\"\n",
      "\"app_reviews_categorize_rating_using_review\"\n",
      "\"cot_sensemaking\"\n",
      "\"trec_1_0_0\"\n",
      "\"super_glue_wic_1_0_2\"\n",
      "\"ropes_prompt_bottom_no_hint\"\n",
      "\"quartz_answer_question_based_on\"\n",
      "\"super_glue_record_1_0_2\"\n",
      "\"yelp_polarity_reviews_0_2_0\"\n",
      "\"race_middle_Is_this_the_right_answer\"\n",
      "\"quoref_Context_Contains_Answer\"\n",
      "\"cos_e_v1_11_rationale\"\n",
      "\"natural_questions_open_1_0_0\"\n",
      "\"ropes_plain_background_situation\"\n",
      "\"web_questions_whats_the_answer\"\n",
      "\"race_high_Read_the_article_and_answer_the_question_no_option_\"\n",
      "\"anli_r3_0_1_0\"\n",
      "\"duorc_SelfRC_generate_question_by_answer\"\n",
      "\"quoref_Find_Answer\"\n",
      "\"duorc_ParaphraseRC_movie_director\"\n",
      "\"sciq_Direct_Question_Closed_Book_\"\n",
      "\"qasc_qa_with_separated_facts_3\"\n",
      "\"lambada_1_0_0\"\n",
      "\"quartz_given_the_fact_answer_the_q\"\n",
      "\"super_glue_cb_1_0_2\"\n",
      "\"quartz_answer_question_below\"\n",
      "\"duorc_ParaphraseRC_answer_question\"\n",
      "\"wmt16_translate_ro_en_1_0_0\"\n",
      "\"dream_generate_last_utterance\"\n",
      "\"wiki_qa_Topic_Prediction_Answer_Only\"\n",
      "\"kilt_tasks_hotpotqa_final_exam\"\n",
      "\"glue_cola_2_0_0\"\n",
      "\"race_high_Select_the_best_answer_no_instructions_\"\n",
      "\"quail_context_description_question_answer_id\"\n",
      "\"ag_news_subset_1_0_0\"\n",
      "\"paws_wiki_1_1_0\"\n",
      "\"sciq_Multiple_Choice\"\n",
      "\"wiki_qa_Direct_Answer_to_Question\"\n",
      "\"gem_dart_1_1_0\"\n",
      "\"cos_e_v1_11_generate_explanation_given_text\"\n",
      "\"wiki_hop_original_generate_object\"\n",
      "\"race_high_Taking_a_test\"\n",
      "\"wiqa_what_might_be_the_last_step_of_the_process\"\n",
      "\"wiki_bio_key_content\"\n",
      "\"quoref_Found_Context_Online\"\n",
      "\"super_glue_wsc_fixed_1_0_2\"\n",
      "\"wiqa_does_the_supposed_perturbation_have_an_effect\"\n",
      "\"adversarial_qa_droberta_tell_what_it_is\"\n",
      "\"cos_e_v1_11_question_description_option_text\"\n",
      "\"gem_common_gen_1_1_0\"\n",
      "\"quoref_Read_And_Extract_\"\n",
      "\"cot_creak\"\n",
      "\"cot_gsm8k_ii\"\n",
      "\"duorc_ParaphraseRC_title_generation\"\n",
      "\"wiki_qa_Is_This_True_\"\n",
      "\"math_dataset_algebra__linear_1d_1_0_0\"\n",
      "\"unified_qa_science_inst\"\n",
      "\"quartz_use_info_from_question_paragraph\"\n",
      "\"web_questions_question_answer\"\n",
      "\"duorc_ParaphraseRC_decide_worth_it\"\n",
      "\"stream_aqua\"\n",
      "\"dbpedia_14_pick_one_category_for_the_following_text\"\n",
      "\"super_glue_multirc_1_0_2\"\n",
      "\"dbpedia_14_given_a_choice_of_categories_\"\n",
      "\"sciq_Direct_Question\"\n",
      "\"kilt_tasks_hotpotqa_combining_facts\"\n",
      "\"quoref_What_Is_The_Answer\"\n",
      "\"web_questions_short_general_knowledge_q\"\n",
      "\"qasc_qa_with_separated_facts_2\"\n",
      "\"wiqa_which_of_the_following_is_the_supposed_perturbation\"\n",
      "\"cnn_dailymail_3_4_0\"\n",
      "\"duorc_ParaphraseRC_generate_question\"\n",
      "\"race_middle_Select_the_best_answer\"\n",
      "\"kilt_tasks_hotpotqa_straighforward_qa\"\n",
      "\"duorc_SelfRC_build_story_around_qa\"\n",
      "\"adversarial_qa_dbidaf_generate_question\"\n",
      "\"snli_1_1_0\"\n",
      "\"app_reviews_convert_to_rating\"\n",
      "\"wiki_hop_original_choose_best_object_affirmative_3\"\n",
      "\"quail_context_question_description_answer_id\"\n",
      "\"cos_e_v1_11_i_think\"\n",
      "\"quoref_Guess_Title_For_Context\"\n",
      "\"quac_1_0_0\"\n",
      "\"cos_e_v1_11_question_option_description_id\"\n",
      "\"quoref_Answer_Test\"\n",
      "\"wiki_hop_original_choose_best_object_interrogative_1\"\n",
      "\"duorc_SelfRC_question_answering\"\n",
      "\"wiki_hop_original_explain_relation\"\n",
      "\"ropes_new_situation_background_answer\"\n",
      "\"dbpedia_14_given_list_what_category_does_the_paragraph_belong_to\"\n",
      "\"race_high_Is_this_the_right_answer\"\n",
      "\"quail_description_context_question_answer_text\"\n",
      "\"cot_strategyqa\"\n",
      "\"ropes_given_background_situation\"\n",
      "\"quail_context_question_answer_description_text\"\n",
      "\"cot_ecqa_ii\"\n",
      "\"ropes_prompt_bottom_hint_beginning\"\n",
      "\"gem_wiki_lingua_english_en_1_1_0\"\n",
      "\"glue_qqp_2_0_0\"\n",
      "\"fix_punct\"\n",
      "\"wiqa_effect_with_string_answer\"\n",
      "\"adversarial_qa_droberta_based_on\"\n",
      "\"imdb_reviews_plain_text_1_0_0\"\n",
      "\"race_high_Select_the_best_answer_generate_span_\"\n",
      "\"race_middle_Select_the_best_answer_generate_span_\"\n",
      "\"race_middle_Write_a_multi_choice_question_for_the_following_article\"\n",
      "\"quarel_do_not_use\"\n",
      "\"duorc_SelfRC_title_generation\"\n",
      "\"qasc_qa_with_separated_facts_5\"\n",
      "\"wiki_qa_exercise\"\n",
      "\"duorc_ParaphraseRC_generate_question_by_answer\"\n",
      "\"web_questions_get_the_answer\"\n",
      "\"wiki_hop_original_choose_best_object_affirmative_1\"\n",
      "\"duorc_ParaphraseRC_extract_answer\"\n",
      "\"dream_baseline\"\n",
      "\"adversarial_qa_dbert_answer_the_following_q\"\n",
      "\"gigaword_1_2_0\"\n",
      "\"ropes_prompt_beginning\"\n",
      "\"quail_context_question_answer_description_id\"\n",
      "\"duorc_SelfRC_answer_question\"\n",
      "\"kilt_tasks_hotpotqa_complex_question\"\n",
      "\"quartz_having_read_above_passage\"\n",
      "\"quail_context_description_question_answer_text\"\n",
      "\"cos_e_v1_11_question_description_option_id\"\n",
      "\"ropes_read_background_situation\"\n",
      "\"wiki_hop_original_choose_best_object_interrogative_2\"\n",
      "\"dbpedia_14_given_a_list_of_category_what_does_the_title_belong_to\"\n",
      "\"gem_web_nlg_en_1_1_0\"\n",
      "\"adversarial_qa_droberta_question_context_answer\"\n",
      "\"qasc_qa_with_separated_facts_1\"\n",
      "\"wiki_qa_automatic_system\"\n",
      "\"ropes_plain_bottom_hint\"\n",
      "\"duorc_SelfRC_decide_worth_it\"\n",
      "\"duorc_ParaphraseRC_question_answering\"\n",
      "\"cos_e_v1_11_explain_why_human\"\n",
      "\"word_segment\"\n",
      "\"cot_creak_ii\"\n",
      "\"anli_r2_0_1_0\"\n",
      "\"cos_e_v1_11_description_question_option_text\"\n",
      "\"quarel_heres_a_story\"\n",
      "\"qasc_qa_with_combined_facts_1\"\n",
      "\"app_reviews_generate_review\"\n",
      "\"wiki_bio_what_content\"\n",
      "\"race_high_Write_a_multi_choice_question_for_the_following_article\"\n",
      "\"qasc_is_correct_1\"\n",
      "\"quoref_Answer_Question_Given_Context\"\n",
      "\"squad_v2_0_3_0_0\"\n",
      "\"web_questions_potential_correct_answer\"\n",
      "\"trivia_qa_rc_1_1_0\"\n",
      "\"wmt16_translate_de_en_1_0_0\"\n",
      "\"cos_e_v1_11_description_question_option_id\"\n",
      "\"wiki_hop_original_generate_subject\"\n",
      "\"ropes_plain_no_background\"\n",
      "\"quarel_choose_between\"\n",
      "\"stream_qed_ii\"\n",
      "\"wiki_bio_guess_person\"\n",
      "\"anli_r1_0_1_0\"\n",
      "\"quail_context_description_question_text\"\n",
      "\"cot_ecqa\"\n",
      "\"quail_context_question_description_answer_text\"\n",
      "\"wiki_bio_who\"\n",
      "\"wiki_qa_Topic_Prediction_Question_Only\"\n",
      "\"glue_stsb_2_0_0\"\n",
      "\"cos_e_v1_11_aligned_with_common_sense\"\n",
      "\"aeslc_1_0_0\"\n",
      "\"dream_generate_first_utterance\"\n",
      "\"wmt16_translate_fi_en_1_0_0\"\n",
      "\"adversarial_qa_dbidaf_answer_the_following_q\"\n",
      "\"dream_answer_to_dialogue\"\n",
      "\"glue_qnli_2_0_0\"\n",
      "\"adversarial_qa_droberta_answer_the_following_q\"\n",
      "\"cot_sensemaking_ii\"\n",
      "\"adversarial_qa_dbert_tell_what_it_is\"\n",
      "\"glue_mnli_2_0_0\"\n",
      "\"quail_description_context_question_text\"\n",
      "\"super_glue_copa_1_0_2\"\n",
      "\"social_i_qa_Check_if_a_random_answer_is_valid_or_not\"\n",
      "\"social_i_qa_Generate_the_question_from_the_answer\"\n",
      "\"social_i_qa_Show_choices_and_generate_index\"\n",
      "\"kilt_tasks_hotpotqa_formulate\"\n",
      "\"gem_e2e_nlg_1_1_0\"\n",
      "\"para_crawl_enes\"\n",
      "\"duorc_SelfRC_extract_answer\"\n",
      "\"sciq_Multiple_Choice_Closed_Book_\"\n",
      "\"race_high_Write_a_multi_choice_question_options_given_\"\n",
      "\"race_middle_Taking_a_test\"\n",
      "\"social_i_qa_I_was_wondering\"\n",
      "\"adversarial_qa_dbert_generate_question\"\n",
      "\"quoref_Guess_Answer\"\n",
      "\"race_middle_Write_a_multi_choice_question_options_given_\"\n",
      "\"quartz_use_info_from_paragraph_question\"\n",
      "\"quoref_Answer_Friend_Question\"\n",
      "\"qasc_is_correct_2\"\n",
      "\"wmt14_translate_fr_en_1_0_0\"\n",
      "\"quarel_testing_students\"\n",
      "\"wiki_hop_original_choose_best_object_affirmative_2\"\n",
      "\"qasc_qa_with_separated_facts_4\"\n",
      "\"duorc_SelfRC_movie_director\"\n",
      "\"wiki_qa_Topic_Prediction_Question_and_Answer_Pair\"\n",
      "\"cosmos_qa_1_0_0\"\n",
      "\"cot_esnli_ii\"\n",
      "\"quail_no_prompt_id\"\n",
      "\"wmt16_translate_tr_en_1_0_0\"\n",
      "\"wiki_qa_Decide_good_answer\"\n",
      "\"wiki_qa_Jeopardy_style\"\n",
      "\"adversarial_qa_dbert_based_on\"\n",
      "\"duorc_SelfRC_generate_question\"\n",
      "\"wiki_qa_Generate_Question_from_Topic\"\n",
      "\"wiki_hop_original_generate_subject_and_object\"\n",
      "\"adversarial_qa_dbidaf_based_on\"\n",
      "\"wiqa_what_is_the_missing_first_step\"\n",
      "\"quartz_read_passage_below_choose\"\n",
      "\"definite_pronoun_resolution_1_1_0\"\n",
      "\"quail_no_prompt_text\"\n",
      "\"wiqa_effect_with_label_answer\"\n",
      "\"drop_2_0_0\"\n",
      "\"race_middle_Select_the_best_answer_no_instructions_\"\n",
      "\"glue_wnli_2_0_0\"\n",
      "\"wiki_bio_comprehension\"\n",
      "\"glue_mrpc_2_0_0\"\n",
      "\"cot_qasc\"\n",
      "\"adversarial_qa_dbert_question_context_answer\"\n",
      "\"quoref_Given_Context_Answer_Question\"\n",
      "\"coqa_1_0_0\"\n",
      "\"quartz_paragraph_question_plain_concat\"\n",
      "\"adversarial_qa_dbidaf_tell_what_it_is\"\n",
      "\"ropes_prompt_mix\"\n",
      "\"social_i_qa_Generate_answer\"\n",
      "\"cot_strategyqa_ii\"\n",
      "\"quarel_logic_test\"\n",
      "\"duorc_ParaphraseRC_build_story_around_qa\"\n",
      "\"stream_aqua_ii\"\n",
      "\"multi_news_1_0_0\"\n",
      "\"ropes_background_situation_middle\"\n",
      "\"sciq_Multiple_Choice_Question_First\"\n",
      "\"squad_v1_1_3_0_0\"\n"
     ]
    }
   ],
   "source": [
    "for name, exp in expert_lib.data.items():\n",
    "    # print task\n",
    "    print(f\"\\\"{exp.expert_task_name}\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings():\n",
    "    svd_embedder = SVDEmbeddingTransform(\n",
    "        SVDEmbeddingTransformConfig(sparsity_threshold=0.5),\n",
    "        random_state=42,\n",
    "    )\n",
    "    embeddings, svd = svd_embedder.transform(expert_lib, upload_to_hf=True, force=True)\n",
    "    del svd_embedder\n",
    "    return embeddings, svd\n",
    "\n",
    "\n",
    "embeds = expert_lib.get_auxiliary_data(\"embeddings\")\n",
    "if len(embeds) == 0:\n",
    "    print(\"creating embeddings\")\n",
    "    _, svd = create_embeddings()\n",
    "\n",
    "# module to embedding\n",
    "module2embed = {}\n",
    "for n, m in expert_lib.items():\n",
    "    module2embed[n] = get_svd_embedding(expert_lib, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the embeddings as a numpy array\n",
    "embeddings = np.array(list(module2embed.values()))\n",
    "cosine_sim_matrix = cosine_similarity(embeddings, embeddings)\n",
    "K = 10\n",
    "kmeans = KMeans(n_clusters=K, init=\"k-means++\", n_init=10, random_state=42)\n",
    "kmeans.fit(cosine_sim_matrix)\n",
    "cluster_labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 4 has 45 elements\n",
      "c4o10_2e = ['glue_sst2_2_0_0', 'dream_read_the_following_conversation_and_answer_the_question', 'race_middle_Read_the_article_and_answer_the_question_no_option_', 'race_high_Select_the_best_answer', 'true_case', 'quail_description_context_question_answer_id', 'quail_context_question_description_text', 'stream_qed', 'cot_esnli', 'quoref_Context_Contains_Answer', 'race_high_Read_the_article_and_answer_the_question_no_option_', 'duorc_ParaphraseRC_movie_director', 'quail_context_description_question_answer_id', 'ag_news_subset_1_0_0', 'wiki_hop_original_generate_object', 'race_high_Taking_a_test', 'cos_e_v1_11_question_description_option_text', 'kilt_tasks_hotpotqa_combining_facts', 'race_middle_Select_the_best_answer', 'kilt_tasks_hotpotqa_straighforward_qa', 'quail_context_question_description_answer_id', 'quac_1_0_0', 'wiki_hop_original_explain_relation', 'quail_context_question_answer_description_text', 'race_high_Select_the_best_answer_generate_span_', 'race_middle_Select_the_best_answer_generate_span_', 'dream_baseline', 'adversarial_qa_dbert_answer_the_following_q', 'quail_context_question_answer_description_id', 'app_reviews_generate_review', 'wiki_bio_what_content', 'quoref_Answer_Question_Given_Context', 'trivia_qa_rc_1_1_0', 'wiki_bio_guess_person', 'glue_stsb_2_0_0', 'adversarial_qa_dbidaf_answer_the_following_q', 'adversarial_qa_droberta_answer_the_following_q', 'kilt_tasks_hotpotqa_formulate', 'gem_e2e_nlg_1_1_0', 'race_middle_Taking_a_test', 'duorc_SelfRC_movie_director', 'quail_no_prompt_id', 'wiki_hop_original_generate_subject_and_object', 'race_middle_Select_the_best_answer_no_instructions_', 'squad_v1_1_3_0_0']\n",
      "Cluster 3 has 32 elements\n",
      "c3o10_2e = ['adversarial_qa_droberta_generate_question', 'wiqa_what_is_the_final_step_of_the_following_process', 'yelp_polarity_reviews_0_2_0', 'cos_e_v1_11_rationale', 'anli_r3_0_1_0', 'duorc_SelfRC_generate_question_by_answer', 'wmt16_translate_ro_en_1_0_0', 'gem_dart_1_1_0', 'cos_e_v1_11_generate_explanation_given_text', 'wiqa_what_might_be_the_last_step_of_the_process', 'gem_common_gen_1_1_0', 'duorc_ParaphraseRC_title_generation', 'cos_e_v1_11_i_think', 'gem_wiki_lingua_english_en_1_1_0', 'duorc_SelfRC_title_generation', 'duorc_ParaphraseRC_generate_question_by_answer', 'gigaword_1_2_0', 'kilt_tasks_hotpotqa_complex_question', 'gem_web_nlg_en_1_1_0', 'cos_e_v1_11_explain_why_human', 'word_segment', 'anli_r2_0_1_0', 'wmt16_translate_de_en_1_0_0', 'anli_r1_0_1_0', 'cos_e_v1_11_aligned_with_common_sense', 'aeslc_1_0_0', 'wmt16_translate_fi_en_1_0_0', 'race_high_Write_a_multi_choice_question_options_given_', 'adversarial_qa_dbert_generate_question', 'race_middle_Write_a_multi_choice_question_options_given_', 'wmt14_translate_fr_en_1_0_0', 'cot_esnli_ii']\n",
      "Cluster 7 has 26 elements\n",
      "c7o10_2e = ['adversarial_qa_dbidaf_question_context_answer', 'quoref_Find_Answer', 'duorc_ParaphraseRC_answer_question', 'quoref_Found_Context_Online', 'adversarial_qa_droberta_tell_what_it_is', 'quoref_Read_And_Extract_', 'duorc_ParaphraseRC_decide_worth_it', 'quoref_What_Is_The_Answer', 'quoref_Guess_Title_For_Context', 'quoref_Answer_Test', 'duorc_SelfRC_question_answering', 'adversarial_qa_droberta_based_on', 'duorc_ParaphraseRC_extract_answer', 'duorc_SelfRC_answer_question', 'adversarial_qa_droberta_question_context_answer', 'duorc_SelfRC_decide_worth_it', 'duorc_ParaphraseRC_question_answering', 'adversarial_qa_dbert_tell_what_it_is', 'duorc_SelfRC_extract_answer', 'quoref_Guess_Answer', 'quoref_Answer_Friend_Question', 'adversarial_qa_dbert_based_on', 'adversarial_qa_dbidaf_based_on', 'adversarial_qa_dbert_question_context_answer', 'quoref_Given_Context_Answer_Question', 'adversarial_qa_dbidaf_tell_what_it_is']\n",
      "Cluster 1 has 57 elements\n",
      "c1o10_2e = ['app_reviews_convert_to_star_rating', 'cos_e_v1_11_question_option_description_text', 'wiki_qa_found_on_google', 'social_i_qa_Show_choices_and_generate_answer', 'app_reviews_categorize_rating_using_review', 'super_glue_wic_1_0_2', 'super_glue_record_1_0_2', 'race_middle_Is_this_the_right_answer', 'sciq_Direct_Question_Closed_Book_', 'qasc_qa_with_separated_facts_3', 'race_high_Select_the_best_answer_no_instructions_', 'sciq_Multiple_Choice', 'wiqa_does_the_supposed_perturbation_have_an_effect', 'math_dataset_algebra__linear_1d_1_0_0', 'dbpedia_14_pick_one_category_for_the_following_text', 'dbpedia_14_given_a_choice_of_categories_', 'sciq_Direct_Question', 'qasc_qa_with_separated_facts_2', 'wiqa_which_of_the_following_is_the_supposed_perturbation', 'app_reviews_convert_to_rating', 'cos_e_v1_11_question_option_description_id', 'dbpedia_14_given_list_what_category_does_the_paragraph_belong_to', 'race_high_Is_this_the_right_answer', 'wiqa_effect_with_string_answer', 'quarel_do_not_use', 'qasc_qa_with_separated_facts_5', 'wiki_qa_exercise', 'ropes_prompt_beginning', 'quartz_having_read_above_passage', 'cos_e_v1_11_question_description_option_id', 'dbpedia_14_given_a_list_of_category_what_does_the_title_belong_to', 'qasc_qa_with_separated_facts_1', 'wiki_qa_automatic_system', 'cos_e_v1_11_description_question_option_text', 'quarel_heres_a_story', 'qasc_qa_with_combined_facts_1', 'qasc_is_correct_1', 'squad_v2_0_3_0_0', 'cos_e_v1_11_description_question_option_id', 'quarel_choose_between', 'social_i_qa_Check_if_a_random_answer_is_valid_or_not', 'social_i_qa_Generate_the_question_from_the_answer', 'social_i_qa_Show_choices_and_generate_index', 'sciq_Multiple_Choice_Closed_Book_', 'social_i_qa_I_was_wondering', 'qasc_is_correct_2', 'quarel_testing_students', 'qasc_qa_with_separated_facts_4', 'wiki_qa_Topic_Prediction_Question_and_Answer_Pair', 'wiki_qa_Decide_good_answer', 'quartz_read_passage_below_choose', 'definite_pronoun_resolution_1_1_0', 'wiqa_effect_with_label_answer', 'drop_2_0_0', 'social_i_qa_Generate_answer', 'quarel_logic_test', 'sciq_Multiple_Choice_Question_First']\n",
      "Cluster 5 has 34 elements\n",
      "c5o10_2e = ['super_glue_rte_1_0_2', 'trec_1_0_0', 'natural_questions_open_1_0_0', 'lambada_1_0_0', 'super_glue_cb_1_0_2', 'wiki_qa_Topic_Prediction_Answer_Only', 'kilt_tasks_hotpotqa_final_exam', 'glue_cola_2_0_0', 'paws_wiki_1_1_0', 'wiki_qa_Direct_Answer_to_Question', 'super_glue_wsc_fixed_1_0_2', 'cot_gsm8k_ii', 'wiki_qa_Is_This_True_', 'unified_qa_science_inst', 'stream_aqua', 'super_glue_multirc_1_0_2', 'snli_1_1_0', 'cot_strategyqa', 'cot_ecqa_ii', 'glue_qqp_2_0_0', 'cot_creak_ii', 'stream_qed_ii', 'wiki_qa_Topic_Prediction_Question_Only', 'glue_qnli_2_0_0', 'cot_sensemaking_ii', 'glue_mnli_2_0_0', 'super_glue_copa_1_0_2', 'wiki_qa_Jeopardy_style', 'wiki_qa_Generate_Question_from_Topic', 'glue_wnli_2_0_0', 'glue_mrpc_2_0_0', 'cot_qasc', 'cot_strategyqa_ii', 'stream_aqua_ii']\n",
      "Cluster 0 has 20 elements\n",
      "c0o10_2e = ['wiqa_what_might_be_the_first_step_of_the_process', 'duorc_ParaphraseRC_generate_question', 'adversarial_qa_dbidaf_generate_question', 'wiki_hop_original_choose_best_object_affirmative_3', 'wiki_hop_original_choose_best_object_interrogative_1', 'quail_description_context_question_answer_text', 'fix_punct', 'imdb_reviews_plain_text_1_0_0', 'wiki_hop_original_choose_best_object_affirmative_1', 'quail_context_description_question_answer_text', 'wiki_hop_original_choose_best_object_interrogative_2', 'wiki_hop_original_generate_subject', 'quail_context_description_question_text', 'quail_context_question_description_answer_text', 'quail_description_context_question_text', 'wiki_hop_original_choose_best_object_affirmative_2', 'cosmos_qa_1_0_0', 'duorc_SelfRC_generate_question', 'wiqa_what_is_the_missing_first_step', 'quail_no_prompt_text']\n",
      "Cluster 8 has 10 elements\n",
      "c8o10_2e = ['huggingface_xsum', 'cot_gsm8k', 'cot_sensemaking', 'cot_creak', 'cnn_dailymail_3_4_0', 'race_middle_Write_a_multi_choice_question_for_the_following_article', 'race_high_Write_a_multi_choice_question_for_the_following_article', 'dream_generate_first_utterance', 'para_crawl_enes', 'wmt16_translate_tr_en_1_0_0']\n",
      "Cluster 9 has 17 elements\n",
      "c9o10_2e = ['ropes_background_new_situation_answer', 'ropes_prompt_bottom_no_hint', 'quartz_answer_question_based_on', 'ropes_plain_background_situation', 'quartz_given_the_fact_answer_the_q', 'quartz_answer_question_below', 'quartz_use_info_from_question_paragraph', 'ropes_new_situation_background_answer', 'ropes_given_background_situation', 'ropes_prompt_bottom_hint_beginning', 'ropes_read_background_situation', 'ropes_plain_bottom_hint', 'ropes_plain_no_background', 'quartz_use_info_from_paragraph_question', 'quartz_paragraph_question_plain_concat', 'ropes_prompt_mix', 'ropes_background_situation_middle']\n",
      "Cluster 6 has 5 elements\n",
      "c6o10_2e = ['web_questions_whats_the_answer', 'web_questions_question_answer', 'web_questions_short_general_knowledge_q', 'web_questions_get_the_answer', 'web_questions_potential_correct_answer']\n",
      "Cluster 2 has 10 elements\n",
      "c2o10_2e = ['dream_generate_last_utterance', 'wiki_bio_key_content', 'duorc_SelfRC_build_story_around_qa', 'cot_ecqa', 'wiki_bio_who', 'dream_answer_to_dialogue', 'wiki_bio_comprehension', 'coqa_1_0_0', 'duorc_ParaphraseRC_build_story_around_qa', 'multi_news_1_0_0']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'\\x03q\\x82\\x93\\x863vb\\xeeZ\\xdda\\xf6\\xe7\\xc5\\xfb\\xd5\\x92 \\x1d\\n\\xafOv\\x833\\x93\\x8e\\xc9d\\x92;\\xbc\\x1fz\\xa5v\\x801\\xdcx\\xbd\\xf0p\\xd6,[\\x072D\\xa6\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\\x01\\x00+\\x00\\x03\\x02\\x03\\x04\\x00-\\x00']\n",
      "Bad pipe message: %s [b':yq[\\x1b\\xf5M\\x80\\xc6\\x1eB=\\x03~X\\x91\\xcc\\x1f \\x7f\\x14$B\\xed\\xe3\\n\\x11o\\xde\\x83G\\xea\\xda1\\x95T\\xa8@@f\\x86\\x11b\\x12x\\x11f\\x9fo\\x99\\x01\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01', b'\\x00+\\x00\\x03\\x02', b'\\x00-']\n",
      "Bad pipe message: %s [b'\\xe1c\\xa4G+\\x10\\xdc\\x99\\x16/\"{D\\xc3M\\xe4\\xeb]\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0\\'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00']\n",
      "Bad pipe message: %s [b'\\xc0.\\xbc\\x03\\x84\\xc7$\\xe2']\n",
      "Bad pipe message: %s [b\"/\\x0e\\xf6\\xf2\\x80*\\xf6q\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\"]\n",
      "Bad pipe message: %s [b'\\xb3\\xfa\\xbd\\xe5']\n",
      "Bad pipe message: %s [b'\\xe9\\xe4\\xc99\\xd1L\\x9fT\\xb4p\\xcdf\\xb6\\x19`\\xfc\\xcb\\x04\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0']\n",
      "Bad pipe message: %s [b'\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x00']\n",
      "Bad pipe message: %s [b'0\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0']\n",
      "Bad pipe message: %s [b'\\x05']\n",
      "Bad pipe message: %s [b'\\x84\\xd0\\xa2##9\\xe2\\x07^C\\x0c\\xfd\\x158<k_\\t\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10']\n",
      "Bad pipe message: %s [b'\\x86\\x17\\n7E\\x08\\x9eu\\x8b\\xb1\\xe7mV!\\rP\\xde\\xf5\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00', b'\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00']\n",
      "Bad pipe message: %s [b'\\x17\\x00\\x03\\xc0\\x10']\n",
      "Bad pipe message: %s [b\"\\x9c_\\xf0e\\xe4\\xf1\\xa4\\xc4o\\x7f\\xba/\\xdc\\xe7_\\x84\\xdd\\xec\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00g\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\"]\n",
      "Bad pipe message: %s [b'\\x84\\xbd\\xbeYi.\\xde^_QW\\x98K\\xf9\\xb217\\xd4\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89', b\"\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x00\\xa6\\x00l\\x004\\x00\\x9b\\x00F\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00;\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\"]\n"
     ]
    }
   ],
   "source": [
    "clusters = defaultdict(list)\n",
    "# Print the cluster labels for each embedding\n",
    "for key, label in zip(module2embed.keys(), cluster_labels):\n",
    "    clusters[label].append(key)\n",
    "\n",
    "for c, l in clusters.items():\n",
    "    print(f\"Cluster {c} has {len(l)} elements\")\n",
    "    print(f\"c{c}o{K}_2e = {l}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rand score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47611902409439244"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_rand_score(cluster_labels, cluster_labels_direct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save svd file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_api_key = os.environ[\"HF_TOKEN\"]\n",
    "login(token=hf_api_key)\n",
    "user = HfApi(token=hf_api_key).whoami()\n",
    "hf_repo_id = \"ostapeno/library-phi_2-v3-10-flan-clusters\"\n",
    "local_lib_location = f\"/tmp/{hf_repo_id}\"\n",
    "if not os.path.exists(local_lib_location):\n",
    "    os.makedirs(local_lib_location)\n",
    "    expert_lib: LocalExpertLibrary = LocalExpertLibrary.create_from_remote(\n",
    "        HFExpertLibrary(hf_repo_id), local_lib_location\n",
    "    )\n",
    "else:\n",
    "    expert_lib: LocalExpertLibrary = LocalExpertLibrary(local_lib_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, svd = create_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload embeddings\n",
    "remote_lib = HFExpertLibrary.from_local(\n",
    "    expert_lib,\n",
    "    hf_repo_id,\n",
    "    force=True,\n",
    "    upload_aux_data=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# To save\n",
    "pickle.dump(svd, open(\"svd.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLayground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp_3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
