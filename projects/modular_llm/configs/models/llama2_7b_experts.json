{
    "num_train_epochs": 5,
    "load_in_8bit": 0,
    "weight_decay": 0.00,
    "learning_rate": 0.0003,
    "micro_batch_size": 8,
    "train_batch_size": 16,
    "predict_batch_size": 1,
    "max_input_length": 1024,
    "max_output_length": 1024,
    "model": "meta-llama/Llama-2-7b-hf",
    "model_family": "gpt",
    "optimizer": "adamw",
    "eval_every": 2,
    "warmup_steps": 100,
    "precision": "bf16"
}